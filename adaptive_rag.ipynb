{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88ab815",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Aiming to use both query analysis and active/self-corrective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-google-genai langchain-huggingface langchainhub chromadb langchain langgraph tavily-python sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = input(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa2b5",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "Setting up a vector database using **HuggingFace** for embeddings(free, the model will be cached to your machine) and **Chroma vector database**. Data will be retrieved directly from the URLs specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe6defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Setting up embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load the documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# Flattening the docs into docs_list. From - [[doc1], [doc2], [doc3]] to [doc1, doc2, doc3]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# This single line double for loop is equivalent to -\n",
    "\"\"\"\n",
    "docs_list = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        docs_list.append(item)\n",
    "\"\"\"\n",
    "\n",
    "# Splitting the documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag-chroma\",\n",
    "    embedding = embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986289f",
   "metadata": {},
   "source": [
    "### Query Analysis via a Router\n",
    "\n",
    "In the prompt we need to define what topics should be redirected to the RAG.\n",
    "\n",
    "This process is kept manual as of now. We can make this automatic and let the llm summarize the RAG and define the prompt\n",
    "for our Router but this can become very expensive for large documents. So in our case since we're only learning and experimenting\n",
    "I've kept this manual for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a39f5e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"I want a structured data object named RouteQuery that must follow certain rules\"\"\"\n",
    "    \"\"\"Route user query\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user query choose to route it to a web search or a vector store\"\n",
    "    )\n",
    "\n",
    "# Defining our llm\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    google_api_key = os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature = 0\n",
    ")\n",
    "# Testing that llm was setup correctly using this\n",
    "# response = llm.invoke(\"Hi How are you\")\n",
    "# print(response.content)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Defining system prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Defining our chain, query will be used to call route_prompt, the output of route_prompt will\n",
    "# be fed into the llm\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n",
    "print(question_router.invoke({\"question\": \"Who won the FIFA worldcup in 2022?\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
