{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88ab815",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Aiming to use both query analysis and active/self-corrective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-google-genai langchain-huggingface langchainhub chromadb langchain langgraph tavily-python sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = input(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa2b5",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "Setting up a vector database using **HuggingFace** for embeddings(free, the model will be cached to your machine) and **Chroma vector database**. Data will be retrieved directly from the URLs specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe6defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alamf\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Setting up embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load the documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# Flattening the docs into docs_list. From - [[doc1], [doc2], [doc3]] to [doc1, doc2, doc3]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# This single line double for loop is equivalent to -\n",
    "\"\"\"\n",
    "docs_list = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        docs_list.append(item)\n",
    "\"\"\"\n",
    "\n",
    "# Splitting the documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag-chroma\",\n",
    "    embedding = embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986289f",
   "metadata": {},
   "source": [
    "#### **Query Analysis via a Router**\n",
    "\n",
    "In the prompt we need to define what topics should be redirected to the RAG.\n",
    "\n",
    "This process is kept manual as of now. We can make this automatic and let the llm summarize the RAG and define the prompt\n",
    "for our Router but this can become very expensive for large documents. So in our case since we're only learning and experimenting\n",
    "I've kept this manual for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39f5e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"I want a structured data object named RouteQuery that must follow certain rules\"\"\"\n",
    "    \"\"\"Route user query\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user query choose to route it to a web search or a vector store\"\n",
    "    )\n",
    "\n",
    "# Defining our llm\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    google_api_key = os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature = 0\n",
    ")\n",
    "# Testing that llm was setup correctly using this\n",
    "# response = llm.invoke(\"Hi How are you\")\n",
    "# print(response.content)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Defining system prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Defining our chain, query will be used to call route_prompt, the output of route_prompt will\n",
    "# be fed into the llm\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n",
    "print(question_router.invoke({\"question\": \"Who won the FIFA worldcup in 2022?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbeb915",
   "metadata": {},
   "source": [
    "#### **Retrieval Grader**\n",
    "\n",
    "After performing the retrieval, we'll evaluate the results. This is just a second check, even though we chose RAG based\n",
    "on the query we'll still make sure that the document content retrieved are sufficiently relevant to the query.\n",
    "\n",
    "Again we'll let the llm decide, its output will be a binary yes or no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866435b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent memory\n",
      "binary_score='yes'\n",
      "Mercedes benz\n",
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description = \"Documents are relevant to the question, 'yes' or 'no\"\n",
    "    )\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# System prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_router\n",
    "\n",
    "# Testing the retrieval grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))\n",
    "\n",
    "question = \"Mercedes benz\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38d83cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Agent memory in an LLM-powered autonomous agent system consists of:\n",
       "\n",
       "*   **Short-term memory:** This is utilized through in-context learning.\n",
       "*   **Long-term memory:** This provides the agent with the capability to retain and recall information over extended periods, often by leveraging an external vector store and fast retrieval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant for question-answering tasks.\n",
    "Use the following retrieved context to answer the user's question.\n",
    "\n",
    "If you don't find the answer in the context, say you don't know — do not make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", human),\n",
    "])\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "docs_txt = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "generate_rag_chain = generate_prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = generate_rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "\n",
    "display(Markdown(generation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee5641",
   "metadata": {},
   "source": [
    "### **Hallucination Grader**\n",
    "\n",
    "This agent will verify if the LLMs produced any hallucinations while producing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee046db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(description = \"Grounded answer in the facts, 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "])\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffcca3",
   "metadata": {},
   "source": [
    "### **Answer Grader**\n",
    "\n",
    "Finally evaluate the answer.\n",
    "\n",
    "Quick Recap - \n",
    "Steps we've followed so far as a part of adaptive RAG. \n",
    "\n",
    "Query analysis(RAG or web_search) -> Retrieval Grader(Rag content relevant?) -> Hallucination grader(Whether llm hallucinated by comparing output to the RAG retrieval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff6688f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(description=\"Answers with 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9c553",
   "metadata": {},
   "source": [
    "### **Question Rewriting**\n",
    "\n",
    "The user's question might not be in a suitable form to query the RAG. To improve the retrieval\n",
    "we'll rephrase the question to ensure it gives better results with the vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0883f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explain the concept of agent memory, including its types and how it functions in artificial intelligence systems.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Original question - \"agent memory\"\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8e0da",
   "metadata": {},
   "source": [
    "### **Web Search Tool**\n",
    "\n",
    "Use Tavily search tool api to get information from the web. It has 1000 free credits. You can explore different web search tools as well :)\n",
    "\n",
    "FYI - these are the params tavily accepts - \n",
    "\n",
    "search_tool = TavilySearchResults(\\\n",
    "    max_results=10, \\\n",
    "    topic=\"news\",\\\n",
    "    include_answer=True,\\\n",
    "    include_raw_content=True,\\\n",
    "    include_images=True,\\\n",
    "    include_image_descriptions=True,\\\n",
    "    search_depth=\"advanced\",\\\n",
    "    time_range=\"week\",\\\n",
    "    start_date=\"2025-11-01\",\\\n",
    "    end_date=\"2025-11-08\",\\\n",
    "    include_domains=[\"bbc.com\", \"nytimes.com\"],\\\n",
    "    exclude_domains=[\"some-low-quality-site.com\"]\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07207e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers.tavily_search_api import TavilySearchAPIRetriever\n",
    "\n",
    "web_search_tool = TavilySearchAPIRetriever(k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428e633",
   "metadata": {},
   "source": [
    "### **Construct the Graph**\n",
    "\n",
    "Now that we have created all the agents, we'll start with the agenticAI part. Agent - The LLMs, with appropriate prompts we've created.\\\n",
    "Here are some details to keep in handy while creating this graph - \n",
    "\n",
    "| Name                     | Purpose                                  | Inputs                    | Outputs                                         |\n",
    "| ------------------------ | ---------------------------------------- | ------------------------- | ----------------------------------------------- |\n",
    "| **question_router**      | Routes query to RAG or Web               | `question`                | `datasource` → `\"vectorstore\"` / `\"web_search\"` |\n",
    "| **retriever**            | Fetches docs from vectorstore            | `query`                   | List of `Document`                              |\n",
    "| **web_search_tool**      | Fetches info from web (Tavily)           | `query`                   | List of web results                             |\n",
    "| **retrieval_grader**     | Checks if retrieved docs are relevant    | `question`, `document`    | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **generate_rag_chain**   | Generates answer using retrieved context | `context`, `question`     | `generation` (string)                           |\n",
    "| **hallucination_grader** | Checks if answer is grounded in facts    | `documents`, `generation` | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **answer_grader**        | Evaluates if answer resolves question    | `question`, `generation`  | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **question_rewriter**    | Rewrites question for better retrieval   | `question`                | `rewritten_question` (string)                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "246d606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represnts the state of the Graph\"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
