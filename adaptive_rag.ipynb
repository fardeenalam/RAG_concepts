{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88ab815",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Aiming to use both query analysis and active/self-corrective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c9a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain_community tiktoken langchain-google-genai langchain-huggingface langchainhub chromadb langchain langgraph tavily-python sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "621e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = input(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "_set_env(\"SECOND_GEMINI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa2b5",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "Setting up a vector database using **HuggingFace** for embeddings(free, the model will be cached to your machine) and **Chroma vector database**. Data will be retrieved directly from the URLs specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fe6defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Setting up embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load the documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# Flattening the docs into docs_list. From - [[doc1], [doc2], [doc3]] to [doc1, doc2, doc3]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# This single line double for loop is equivalent to -\n",
    "\"\"\"\n",
    "docs_list = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        docs_list.append(item)\n",
    "\"\"\"\n",
    "\n",
    "# Splitting the documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag-chroma\",\n",
    "    embedding = embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986289f",
   "metadata": {},
   "source": [
    "#### **Query Analysis via a Router**\n",
    "\n",
    "In the prompt we need to define what topics should be redirected to the RAG.\n",
    "\n",
    "This process is kept manual as of now. We can make this automatic and let the llm summarize the RAG and define the prompt\n",
    "for our Router but this can become very expensive for large documents. So in our case since we're only learning and experimenting\n",
    "I've kept this manual for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f5e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I don\\'t have feelings or a physical body, so I can\\'t really \"be\" anything in the way humans are! But I\\'m functioning perfectly and ready to help you.\\n\\nHow are **you** doing today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--7d0e778c-060f-41e6-8f02-665d3e93e6ee-0' usage_metadata={'input_tokens': 7, 'output_tokens': 79, 'total_tokens': 86, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 31}}\n",
      "datasource='vectorstore'\n",
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"I want a structured data object named RouteQuery that must follow certain rules\"\"\"\n",
    "    \"\"\"Route user query\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user query choose to route it to a web search or a vector store\"\n",
    "    )\n",
    "\n",
    "hf_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Defining our llm\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    google_api_key = os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature = 0\n",
    ")\n",
    "llm_for_document_check = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    google_api_key = os.getenv(\"SECOND_GEMINI_API_KEY\"),\n",
    "    temperature = 0\n",
    ")\n",
    "# response = llm_for_document_check.invoke(\"Hi! how are you?\")\n",
    "# print(response)\n",
    "# Testing that llm was setup correctly using this\n",
    "# response = llm.invoke(\"Hi How are you\")\n",
    "# print(response.content)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Defining system prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Defining our chain, query will be used to call route_prompt, the output of route_prompt will\n",
    "# be fed into the llm\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n",
    "print(question_router.invoke({\"question\": \"Who won the FIFA worldcup in 2022?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbeb915",
   "metadata": {},
   "source": [
    "#### **Retrieval Grader**\n",
    "\n",
    "After performing the retrieval, we'll evaluate the results. This is just a second check, even though we chose RAG based\n",
    "on the query we'll still make sure that the document content retrieved are sufficiently relevant to the query.\n",
    "\n",
    "Again we'll let the llm decide, its output will be a binary yes or no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "866435b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent memory\n",
      "binary_score='yes'\n",
      "Mercedes benz\n",
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description = \"Documents are relevant to the question, 'yes' or 'no\"\n",
    "    )\n",
    "\n",
    "structured_llm_router = llm_for_document_check.with_structured_output(GradeDocuments)\n",
    "\n",
    "# System prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_router\n",
    "\n",
    "# Testing the retrieval grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))\n",
    "\n",
    "question = \"Mercedes benz\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb04ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"Agents memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38d83cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In an LLM-powered autonomous agent system, memory is a crucial component, categorized into:\n",
       "\n",
       "*   **Short-term memory:** This refers to the in-context learning capabilities of the model, often limited by the prompt's token or word limit (e.g., AutoGPT mentions a ~4000-word limit). It's used for immediate learning within a single interaction or task.\n",
       "*   **Long-term memory:** This allows the agent to retain and recall information over extended periods. It typically leverages an external vector store and fast retrieval mechanisms to store and access a potentially infinite amount of information.\n",
       "\n",
       "For instance, in the Generative Agents simulation, a \"memory stream\" acts as a long-term memory module, recording agents' experiences in natural language within an external database. A \"retrieval model\" then surfaces relevant context from this memory based on recency, importance, and relevance to inform the agent's behavior."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant for question-answering tasks.\n",
    "Use the following retrieved context to answer the user's question.\n",
    "\n",
    "If you don't find the answer in the context, say you don't know — do not make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", human),\n",
    "])\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "generate_rag_chain = generate_prompt | llm | StrOutputParser()\n",
    "\n",
    "docs_txt = format_docs(docs)\n",
    "generation = generate_rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "\n",
    "display(Markdown(generation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee5641",
   "metadata": {},
   "source": [
    "### **Hallucination Grader**\n",
    "\n",
    "This agent will verify if the LLMs produced any hallucinations while producing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee046db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(description = \"Grounded answer in the facts, 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "])\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffcca3",
   "metadata": {},
   "source": [
    "### **Answer Grader**\n",
    "\n",
    "Finally evaluate the answer.\n",
    "\n",
    "Quick Recap - \n",
    "Steps we've followed so far as a part of adaptive RAG. \n",
    "\n",
    "Query analysis(RAG or web_search) -> Retrieval Grader(Rag content relevant?) -> Hallucination grader(Whether llm hallucinated by comparing output to the RAG retrieval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6688f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    binary_score: str = Field(description=\"Answers with 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9c553",
   "metadata": {},
   "source": [
    "### **Question Rewriting**\n",
    "\n",
    "The user's question might not be in a suitable form to query the RAG. To improve the retrieval\n",
    "we'll rephrase the question to ensure it gives better results with the vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0883f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the key concepts, types, and architectural considerations for memory systems in intelligent agents?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"You are a question rewriter that improves user queries for semantic search over a vectorstore.\n",
    "\n",
    "The vectorstore contains documents on:\n",
    "1. LLM-powered autonomous agents (architecture, memory, planning, tool use)\n",
    "2. Prompt engineering (prompt design, in-context learning, chain-of-thought)\n",
    "3. Model red-teaming (adversarial attacks, jailbreaks, safety, robustness)\n",
    "\n",
    "Rephrase the given question so it aligns with these topics and retrieves the most relevant information possible.\n",
    "Focus on clarity, intent, and alignment with the above domains.\"\"\"\n",
    "\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Original question - \"agent memory\"\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8e0da",
   "metadata": {},
   "source": [
    "### **Web Search Tool**\n",
    "\n",
    "Use Tavily search tool api to get information from the web. It has 1000 free credits. You can explore different web search tools as well :)\n",
    "\n",
    "FYI - these are the params tavily accepts - \n",
    "\n",
    "search_tool = TavilySearchResults(\\\n",
    "    max_results=10, \\\n",
    "    topic=\"news\",\\\n",
    "    include_answer=True,\\\n",
    "    include_raw_content=True,\\\n",
    "    include_images=True,\\\n",
    "    include_image_descriptions=True,\\\n",
    "    search_depth=\"advanced\",\\\n",
    "    time_range=\"week\",\\\n",
    "    start_date=\"2025-11-01\",\\\n",
    "    end_date=\"2025-11-08\",\\\n",
    "    include_domains=[\"bbc.com\", \"nytimes.com\"],\\\n",
    "    exclude_domains=[\"some-low-quality-site.com\"]\\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07207e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428e633",
   "metadata": {},
   "source": [
    "### **Construct the Graph**\n",
    "\n",
    "Now that we have created all the agents, we'll start with the agenticAI part. Agent - The LLMs, with appropriate prompts we've created.\\\n",
    "Here are some details to keep in handy while creating this graph - \n",
    "\n",
    "| Name                     | Purpose                                  | Inputs                    | Outputs                                         |\n",
    "| ------------------------ | ---------------------------------------- | ------------------------- | ----------------------------------------------- |\n",
    "| **question_router**      | Routes query to RAG or Web               | `question`                | `datasource` → `\"vectorstore\"` / `\"web_search\"` |\n",
    "| **retriever**            | Fetches docs from vectorstore            | `query`                   | List of `Document`                              |\n",
    "| **web_search_tool**      | Fetches info from web (Tavily)           | `query`                   | List of web results                             |\n",
    "| **retrieval_grader**     | Checks if retrieved docs are relevant    | `question`, `document`    | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **generate_rag_chain**   | Generates answer using retrieved context | `context`, `question`     | `generation` (string)                           |\n",
    "| **hallucination_grader** | Checks if answer is grounded in facts    | `documents`, `generation` | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **answer_grader**        | Evaluates if answer resolves question    | `question`, `generation`  | `binary_score` → `\"yes\"` / `\"no\"`               |\n",
    "| **question_rewriter**    | Rewrites question for better retrieval   | `question`                | `rewritten_question` (string)                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"Represnts the state of the Graph\"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "    retries: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ca8a8",
   "metadata": {},
   "source": [
    "### **Graph Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40618818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def retriever(state):\n",
    "    \"\"\"Retrieve documents, returns -> the documents retrieved and the question\"\"\"\n",
    "\n",
    "    print(\"-------Retrieval step-------\")\n",
    "    # Getting the already populated question variable from state\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    docs = retriever.invoke(question)\n",
    "\n",
    "    if docs and isinstance(docs[0], dict):\n",
    "        docs = [Document(page_content=d.get(\"page_content\", \"\"), metadata=d.get(\"metadata\", {})) for d in docs]\n",
    "\n",
    "    return {\"documents\": docs, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"Generate the answer, returns -> state with generation variable populated\"\"\"\n",
    "    print(\"-------Generation step-------\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    docs_txt = format_docs(documents)\n",
    "    generation = generate_rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Grade whether the retrieved documents are relevant to the question or not\n",
    "        returns -> Appends a score to each doc RELEVANT or NOT RELEVANT\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Now lets score each of the documents\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question. Returns -> a better question state[\"question\"]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    retries = state.get(\"retries\", 0)\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"Web search the query. Returns -> updated documents query with the search results\"\"\"\n",
    "\n",
    "    print(\"-----WEB SEARCH-----\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # web search \n",
    "    response = tavily_client.search(query=question)\n",
    "    if response and \"results\" in response:\n",
    "        web_results_texts = [r[\"content\"] for r in response[\"results\"] if r.get(\"content\")]\n",
    "        joined_text = \"\\n\".join(web_results_texts)\n",
    "        web_results = Document(page_content=joined_text)\n",
    "    else:\n",
    "        web_results = Document(page_content=\"\")\n",
    "\n",
    "    return {\"documents\": [web_results], \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91c727",
   "metadata": {},
   "source": [
    "### **Graph Edges**\n",
    "\n",
    "The previous code block is quite long, but it basically defines all our graph nodes that we'll be using. Now we'll define our graph edges. Graph Edges represent the decision logic that determines which node should run next based on a node’s output or the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48338b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"Route question to web search or RAG. returns -> 'vectorstore' or 'web_search'\"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    \n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"Determines whether to generate the response or to re-generate the question. \n",
    "        Returns -> 'transform_query' or 'generate'\"\"\"\n",
    "    \n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # this means that the grade documents node returned an empty docs_list\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "    \n",
    "\n",
    "def rate_answer(state):\n",
    "    \"\"\"\n",
    "    We'll use hallucination_grader here to check if the llm hallucinated in generating the response first.\n",
    "    Second we'll use the answer_grader to check if the final response generated was sufficient in \n",
    "    addressing the user's query\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Checking for hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: NO HALLUCINATIONS---\")\n",
    "        # Checking for the answer relevance now\n",
    "        print(\"GRADING GENERATION VS. QUESTION\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: LLM HALLUCINATED, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f7765",
   "metadata": {},
   "source": [
    "### **Compiling Graph**\n",
    "\n",
    "Now that we've defined all our nodes and edges, we'll compile the final graph, essentially mapping how the nodes connect to each other through edges and defining the overall execution flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d066a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "# StateGraph is the core class in LangGraph used to define a graph of nodes that share and update a common state\n",
    "# our GraphState in this case\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Defining the nodes\n",
    "workflow.add_node(\"web_search\", web_search) \n",
    "workflow.add_node(\"retrieve\", retriever)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "# Building the graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question, # decides web or vectorstore\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    rate_answer,\n",
    "    {\n",
    "        \"not supported\": \"generate\",  # not supported means that the llm hallucinated, so retry generation\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42b674",
   "metadata": {},
   "source": [
    "### **Use Graph**\n",
    "\n",
    "All the steps for this adaptive RAG design are now complete :) \\\n",
    "We'll now look at various examples where we test and see how it works for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "-----WEB SEARCH-----\n",
      "Node 'web_search':\n",
      "\n",
      "---\n",
      "\n",
      "-------Generation step-------\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: NO HALLUCINATIONS---\n",
      "GRADING GENERATION VS. QUESTION\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "During the 2022 FIFA World Cup, Lionel Messi achieved several significant milestones:\n",
      "\n",
      "*   He finally claimed the World Cup trophy with Argentina.\n",
      "*   He broke Diego Maradona's record for most World Cup appearances for Argentina.\n",
      "*   He surpassed Lothar Matthaus for the record of most appearances at the World Cup Finals (26 appearances).\n",
      "*   He matched Maradona's Argentinian record of eight assists at World Cups.\n",
      "*   He scored seven goals, including two in the final.\n",
      "*   He provided three assists.\n",
      "*   He played a starring role in Argentina's victory, fulfilling his lifelong dream.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"question\": \"Summarize Lionel Messi's achievements during the 2022 FIFA World Cup.\"\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a6046fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "-------Retrieval step-------\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "-------Generation step-------\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: NO HALLUCINATIONS---\n",
      "GRADING GENERATION VS. QUESTION\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Based on the provided context, the main type of agent memory described is the **Memory stream**.\n",
      "\n",
      "It is characterized as:\n",
      "*   A long-term memory module (external database).\n",
      "*   It records a comprehensive list of agents’ experiences in natural language.\n",
      "*   Each element is an observation, an event directly provided by the agent.\n",
      "*   Inter-agent communication can trigger new natural language statements within this memory.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"question\": \"What are the types of agent memory?\"\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "839c67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "-------Retrieval step-------\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "---TRANSFORM QUERY---\n",
      "Node 'transform_query':\n",
      "\n",
      "---\n",
      "\n",
      "-------Retrieval step-------\n",
      "Node 'retrieve':\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "\n",
      "---\n",
      "\n",
      "-------Generation step-------\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: NO HALLUCINATIONS---\n",
      "GRADING GENERATION VS. QUESTION\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Node 'generate':\n",
      "\n",
      "---\n",
      "\n",
      "Based on the provided context, prompt engineering strategies that are effective in mitigating or preventing hallucination in large language models include:\n",
      "\n",
      "*   **Internet augmented language models through few-shot prompting for open-domain question answering** [20]. This suggests using external, verifiable information from the internet to ground responses.\n",
      "*   **Program of Thoughts Prompting** [21] and **Program-aided language models (PAL)** [22], which disentangle computation from reasoning for numerical reasoning tasks. This implies offloading computational tasks to external programs to ensure accuracy and prevent the model from generating incorrect numerical results.\n",
      "*   **Tool Augmented Language Models (TALM)** [23], which is a broader category encompassing the use of external tools to enhance the model's capabilities and likely improve factual accuracy.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"question\": \"Can prompt engineering prevent hallucination?\"\n",
    "}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Node '{key}':\")\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
