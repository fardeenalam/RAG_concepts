{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88ab815",
   "metadata": {},
   "source": [
    "# Adaptive RAG\n",
    "\n",
    "Aiming to use both query analysis and active/self-corrective RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_community tiktoken langchain-google-genai langchain-huggingface langchainhub chromadb langchain langgraph tavily-python sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621e7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = input(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GEMINI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaa2b5",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "\n",
    "Setting up a vector database using **HuggingFace** for embeddings(free, the model will be cached to your machine) and **Chroma vector database**. Data will be retrieved directly from the URLs specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe6defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alamf\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Setting up embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load the documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# Flattening the docs into docs_list. From - [[doc1], [doc2], [doc3]] to [doc1, doc2, doc3]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# This single line double for loop is equivalent to -\n",
    "\"\"\"\n",
    "docs_list = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        docs_list.append(item)\n",
    "\"\"\"\n",
    "\n",
    "# Splitting the documents into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Create vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = doc_splits,\n",
    "    collection_name = \"rag-chroma\",\n",
    "    embedding = embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986289f",
   "metadata": {},
   "source": [
    "#### **Query Analysis via a Router**\n",
    "\n",
    "In the prompt we need to define what topics should be redirected to the RAG.\n",
    "\n",
    "This process is kept manual as of now. We can make this automatic and let the llm summarize the RAG and define the prompt\n",
    "for our Router but this can become very expensive for large documents. So in our case since we're only learning and experimenting\n",
    "I've kept this manual for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39f5e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"I want a structured data object named RouteQuery that must follow certain rules\"\"\"\n",
    "    \"\"\"Route user query\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ..., \n",
    "        description=\"Given a user query choose to route it to a web search or a vector store\"\n",
    "    )\n",
    "\n",
    "# Defining our llm\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    google_api_key = os.getenv(\"GEMINI_API_KEY\"),\n",
    "    temperature = 0\n",
    ")\n",
    "# Testing that llm was setup correctly using this\n",
    "# response = llm.invoke(\"Hi How are you\")\n",
    "# print(response.content)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Defining system prompt\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Defining our chain, query will be used to call route_prompt, the output of route_prompt will\n",
    "# be fed into the llm\n",
    "question_router = route_prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n",
    "print(question_router.invoke({\"question\": \"Who won the FIFA worldcup in 2022?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbeb915",
   "metadata": {},
   "source": [
    "#### **Retrieval Grader**\n",
    "\n",
    "After performing the retrieval, we'll evaluate the results. This is just a second check, even though we chose RAG based\n",
    "on the query we'll still make sure that the document content retrieved are sufficiently relevant to the query.\n",
    "\n",
    "Again we'll let the llm decide, its output will be a binary yes or no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866435b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent memory\n",
      "binary_score='yes'\n",
      "Mercedes benz\n",
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description = \"Documents are relevant to the question, 'yes' or 'no\"\n",
    "    )\n",
    "\n",
    "structured_llm_router = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# System prompt \n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "])\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_router\n",
    "\n",
    "# Testing the retrieval grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))\n",
    "\n",
    "question = \"Mercedes benz\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_content = docs[1].page_content\n",
    "print(question)\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_content}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38d83cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In a LLM-powered autonomous agent system, memory is a key component, and it is categorized into two main types:\n",
       "\n",
       "1.  **Short-term memory:** This refers to the in-context learning capabilities of the model, often utilized through prompt engineering.\n",
       "2.  **Long-term memory:** This allows the agent to retain and recall information over extended periods. It often leverages an external vector store and fast retrieval mechanisms to provide access to an \"infinite\" amount of information. Maximum Inner Product Search (MIPS) is mentioned as a method related to long-term memory."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "system = \"\"\"\n",
    "You are a helpful assistant for question-answering tasks.\n",
    "Use the following retrieved context to answer the user's question.\n",
    "\n",
    "If you don't find the answer in the context, say you don't know â€” do not make up an answer.\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", human),\n",
    "])\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "docs_txt = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "generate_rag_chain = generate_prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = generate_rag_chain.invoke({\"context\": docs_txt, \"question\": question})\n",
    "\n",
    "display(Markdown(generation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee5641",
   "metadata": {},
   "source": [
    "### **Hallucination Grader**\n",
    "\n",
    "This agent will verify if the LLMs produced any hallucinations while producing the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee046db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    binary_score: str = Field(description = \"Grounded answer in the facts, 'yes' or 'no'\")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "])\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
